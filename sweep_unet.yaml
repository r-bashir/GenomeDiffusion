# ===== W&B Sweeps Configuration for GenomeDiffusion HPO =====
# Comprehensive hyperparameter optimization for cluster/server
# Full hyperparameter search across all important parameters

# Program to run
program: train_sweep.py

# Optimization method
method: bayes

# Total trials across all agents
run_cap: 50

# Metric to optimize
metric:
  goal: minimize
  name: val_loss_epoch

# Early termination
early_terminate:
  type: hyperband
  min_iter: 10  # More aggressive early stopping
  eta: 5  # Faster elimination of poor trials

# Hyperparameter search space
parameters:

  # === DATA ===
  batch_size:
    values: [16, 32, 64]

  # === UNet1D ===
  embedding_dim:
    values: [128, 256, 512]

  dim_mults:
    values: [[1, 2], [1, 2, 4], [1, 2, 4, 8]]

  with_time_emb:
    values: [true, false]

  time_dim:
    values: [128, 256, 512]

  with_pos_emb:
    values: [true, false]

  pos_dim:
    values: [128, 256, 512]

  edge_pad:
    values: [1, 2, 4]

  norm_groups:
    values: [2, 4, 8, 16]

  dropout:
    values: [0.0, 0.1, 0.2, 0.3]

  use_scale_shift_norm:
    values: [true, false]

  # === ATTENTION ===
  # use_attention:
  #   values: [true, false]

  # attention_heads:
  #   values: [4, 8, 16, 32]

  # attention_dim_head:
  #   values: [16, 32, 64, 128]

  # === TRAINING ===
  epochs:
    value: 50

  accumulate_grad_batches:
    values: [4, 8, 16]

  # === OPTIMIZER ===
  learning_rate:
    # Base LR for non-OneCycle schedulers; for OneCycle, base is max_lr/div_factor
    values: [0.00002, 0.00005, 0.0001, 0.0002, 0.0003, 0.0005]

  weight_decay:
    values: [0.000001, 0.000003, 0.00001, 0.00003, 0.00005, 0.0001, 0.0003, 0.0005, 0.001]

  amsgrad:
    values: [true, false]

  # Additional AdamW hyperparameters
  beta1:
    values: [0.85, 0.9, 0.95]
  beta2:
    values: [0.99, 0.995, 0.999]
  eps:
    values: [1e-8, 3e-8, 1e-7, 3e-7, 1e-6]

  # === SCHEDULER ===
  scheduler_type:
    values: ['onecycle', 'warmup_cosine', 'cosine', 'reduce']

  # OneCycleLR
  scheduler_max_lr:
    # Focus on safe band for resumed diffusion runs
    values: [0.0001, 0.0002, 0.0003, 0.0005, 0.0007, 0.001]

  scheduler_pct_start:
    # Fraction of steps in warmup phase (long cosine tail)
    values: [0.2, 0.25, 0.3, 0.35, 0.4]

  scheduler_anneal_strategy:
    # Annealing strategy for OneCycleLR
    values: ['cos', 'linear']

  scheduler_div_factor:
    # initial_lr = max_lr / div_factor
    values: [10.0, 15.0, 20.0, 25.0, 30.0]

  scheduler_final_div_factor:
    # final_lr = initial_lr / final_div_factor
    values: [50, 100, 200, 400, 800]

  # WarmupCosineLR
  warmup_epochs:
    values: [5, 10, 15]

  # CosineAnnealingLR
  eta_min:
    values: [1e-7, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]

  # ReduceLROnPlateau
  scheduler_mode:
    values: ['min']

  scheduler_factor:
    values: [0.2, 0.3, 0.5, 0.7, 0.8]

  scheduler_patience:
    values: [3, 5, 8, 10, 15, 20, 30]

  scheduler_threshold:
    values: [1e-6, 1e-5, 1e-4, 1e-3]

  scheduler_min_lr:
    values: [1e-7, 1e-6, 1e-5]
