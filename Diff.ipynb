{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb76e34-e7e2-43cc-a5af-9a242cbc7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "\n",
    "# Pytorch Lightening\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# For Visualization\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline     \n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9576363d-9b83-4f06-a0d6-2b74577c7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def load_data(input_path=\"\"):\n",
    "\n",
    "    # read data\n",
    "    data = pd.read_parquet(input_path).to_numpy()\n",
    "\n",
    "    # normalize data: map (0 → 0.0, 1 → 0.5, 2 → 1.0)\n",
    "    data = np.where(data == 0, 0.0, data)  # Map 0 to 0.0\n",
    "    data = np.where(data == 1, 0.5, data)  # Map 1 to 0.5\n",
    "    data = np.where(data == 2, 1.0, data)  # Map 2 to 1.0\n",
    "    \n",
    "    return torch.FloatTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17af7699-f62d-42f4-bf7f-b0890a3fd760",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"HO_data/HO_data_filtered/HumanOrigins2067_filtered.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83718808-8c3b-4558-ab29-44afee202f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data = load_data(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f6777c-c42c-4c8b-ba76-247954144348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([160858, 2067])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5695fac1-fb3b-4d0f-8b2a-e16d560e8a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values after normalization: [0.  0.5 1.  9. ]\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "unique_values = np.unique(snp_data)\n",
    "print(\"Unique values after normalization:\", unique_values)  # Should show [0.0, 0.5, 1.0, 9.0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1928d5a8-7025-4859-b232-97d30cf7f799",
   "metadata": {},
   "source": [
    "# Maybe I don't need it:\n",
    "class SNPDataset(Dataset):\n",
    "    def __init__(self, input_path):\n",
    "        self.data = load_data(input_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]  # Number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  # Return one sample (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581bb64-75cc-4cf9-ac43-e21ad293ebb4",
   "metadata": {},
   "source": [
    "### _LightningDataModule_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b113bb7d-8237-4dec-95e1-dc2426cc2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNPDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, input_path, batch_size=256, num_workers=1):\n",
    "        super().__init__()\n",
    "        self.path = input_path\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = num_workers\n",
    "        self.data_split = [128686, 16086, 16086] # 80%, 10% and 10%\n",
    "\n",
    "    # Setup Data\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Prepare the dataset\"\"\"\n",
    "        full_dataset = load_data(self.path)\n",
    "        self.trainset, self.valset, self.testset = random_split(\n",
    "            full_dataset,\n",
    "            self.data_split,\n",
    "            generator=torch.Generator().manual_seed(42)  # Fixed seed for reproducibility\n",
    "        )\n",
    "\n",
    "    # Data Loaders\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.trainset, batch_size=self.batch_size, shuffle=True, num_workers=self.workers\n",
    "            )  # , pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valset, batch_size=self.batch_size, shuffle=False, num_workers=self.workers\n",
    "            )  # , pin_memory=True, persistent_workers=True)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.testset, batch_size=self.batch_size, shuffle=False, num_workers=self.workers\n",
    "            )  # , pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3c06bc6-85fa-4751-b7ab-d825cb1689cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize DataModule\n",
    "snp_data_module = SNPDataModule(input_path=input_file, batch_size=256, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d3590e-53cd-4785-8f27-aa46c32c0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Data\n",
    "snp_data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85755e59-5235-4f8c-a11d-5d4d2fa5ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "train_loader = snp_data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44982907-9fe9-4717-8168-b10134f212be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from DataLoader\n",
    "sample_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eff3f5bd-c13a-4132-b1bd-bd13164c24c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: torch.Size([256, 2067])\n",
      "First 5 Samples:\n",
      " tensor([[0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5000],\n",
      "        [0.5000, 0.0000, 0.5000,  ..., 0.5000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5000,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch Shape:\", sample_batch.shape)  # Expected: (batch_size, num_markers)\n",
    "print(\"First 5 Samples:\\n\", sample_batch[:5])  # Show first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a839084-3720-4916-82ae-c6af0b7e86a1",
   "metadata": {},
   "source": [
    "### _LightningModule_\n",
    "\n",
    "- Model\n",
    "- Training Hooks (training, validation, testing)\n",
    "- Data Hooks (training, validation, testing)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450663c2-fd77-4d60-95fc-da3e93d09cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkBase(pl.LightningModule):\n",
    "    def __init__(self, input_path, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.path = hparams[\"input_path\"]\n",
    "        self.split = [10, 1, 1]\n",
    "        self.batch = 64\n",
    "        self.workers = 4\n",
    "        self.data_split = [128686, 16086, 16086] # 80%, 10% and 10%\n",
    "        self.trainset, self.valset, self.testset = None, None, None\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "    # Setup Data\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Prepare the dataset\"\"\"\n",
    "        full_dataset = load_data(self.path)\n",
    "        self.trainset, self.valset, self.testset = random_split(\n",
    "            full_dataset,\n",
    "            self.data_split,\n",
    "            generator=torch.Generator().manual_seed(42)  # Fixed seed for reproducibility\n",
    "        )\n",
    "\n",
    "    # Data Loaders\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.trainset, batch_size=self.batch_size, shuffle=True, num_workers=self.workers\n",
    "            )  # , pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valset, batch_size=self.batch_size, shuffle=False, num_workers=self.workers\n",
    "            )  # , pin_memory=True, persistent_workers=True)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.testset, batch_size=self.batch_size, shuffle=False, num_workers=self.workers\n",
    "            )  # , pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    # Configure Optimizer & Scheduler\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure the Optimizer and Scheduler\"\"\"\n",
    "        optimizer = [\n",
    "            torch.optim.AdamW(\n",
    "                self.parameters(),\n",
    "                lr=(self.hparams[\"lr\"]),\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-08,\n",
    "                amsgrad=True,\n",
    "            )\n",
    "        ]\n",
    "        scheduler = [\n",
    "            {\n",
    "                \"scheduler\": torch.optim.lr_scheduler.StepLR(\n",
    "                    optimizer[0],\n",
    "                    step_size=0.3,\n",
    "                    gamma=10,\n",
    "                ),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        ]\n",
    "        return optimizer, scheduler\n",
    "    \n",
    "    # Trainig Step\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # YOUR CODE HERE:\n",
    "        pass\n",
    "\n",
    "    # Validation Step\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # YOUR CODE HERE:\n",
    "        pass\n",
    "\n",
    "    # Test Step\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # YOUR CODE HERE:\n",
    "        pass\n",
    "\n",
    "    # Optimizer Step\n",
    "    def on_before_optimizer_step(self, optimizer, *args, **kwargs):\n",
    "        \"\"\"Settings before Optimizer Step\"\"\"\n",
    "\n",
    "        # warm up lr\n",
    "        if self.hparams.get(\"warmup\", 0) and (\n",
    "            self.trainer.current_epoch < self.hparams[\"warmup\"]\n",
    "        ):\n",
    "            lr_scale = min(\n",
    "                1.0, float(self.trainer.current_epoch + 1) / self.hparams[\"warmup\"]\n",
    "            )\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] = lr_scale * self.hparams[\"lr\"]\n",
    "\n",
    "        # after reaching minimum learning rate, stop LR decay\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = max(pg[\"lr\"], self.hparams.get(\"min_lr\", 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd81b5-ac91-4a6f-a468-0d691a68b93e",
   "metadata": {},
   "source": [
    "### _Model Development_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf89b9-6328-47af-9207-646b5b265833",
   "metadata": {},
   "source": [
    "- These define the range of noise levels.\n",
    "- The noise increases from 1e-4 (almost no noise) to 0.02 (more noise) over time.\n",
    "- `t_range` → Total number of diffusion steps.\n",
    "- `in_size` → Input image size (flattened).\n",
    "- `img_depth` → Number of image channels (e.g., 3 for RGB).\n",
    "- `self.unet` → A U-Net model that predicts the noise at each step.\n",
    "- Simply passes the input image (x) and time step (t) to the `U-Net`.\n",
    "- The `U-Net` predicts the noise (ϵ) at that time step.\n",
    "- `Beta function` Linearly interpolates between beta_small and beta_large over time, Controls how much noise is added at each time step.\n",
    "- `α-alpha function` controls signal preservation. Since β(t) is noise, α(t) = 1 - β(t) represents how much of the original image remains at each step.\n",
    "- `α̅(t)-Cumulative alpha` is the product of all previous α(t) values.Represents the total preservation of the original image after t steps.\n",
    "- `Get_loss` Selects a random diffusion step t for each image in the batch. Generates Gaussian noise ϵ for each image.\n",
    "- Loop: Computes the noisy version of the image using First term: Preserves part of the original image. Second term: Adds noise.\n",
    "- Denoising and Loss calculation: Runs noisy images through the U-Net to predict the noise (e_hat). Loss function: Mean Squared Error (MSE) between: Predicted noise (e_hat) Actual noise (ϵ) This teaches the model to predict noise correctly, enabling image denoising.\n",
    "- `Denoise_sample` Starts from a noisy sample (x_T = pure noise).Generates random Gaussian noise (z) unless it’s the last step.\n",
    "- Gets predicted noise (e_hat) from U-Net. Computes the denoised image (x_{t-1}) using:\n",
    "First term: Restores signal.\n",
    "Second term: Removes predicted noise.\n",
    "Third term: Adds slight randomness (for realistic diversity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5c9f4-c6d1-4372-84c0-0d614a575128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(NetworkBase):\n",
    "    def __init__(self, in_size, t_range, img_depth):\n",
    "        super().__init__()\n",
    "        self.beta_small = 1e-4\n",
    "        self.beta_large = 0.02\n",
    "        self.t_range = t_range\n",
    "        self.in_size = in_size\n",
    "\n",
    "        self.unet = Unet(dim = 64, dim_mults = (1, 2, 4, 8), channels=img_depth)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.unet(x, t)\n",
    "\n",
    "    def beta(self, t):\n",
    "        # Just a simple linear interpolation between beta_small and beta_large based on t\n",
    "        return self.beta_small + (t / self.t_range) * (self.beta_large - self.beta_small)\n",
    "\n",
    "    def alpha(self, t):\n",
    "        return 1 - self.beta(t)\n",
    "\n",
    "    def alpha_bar(self, t):\n",
    "        # Product of alphas from 0 to t\n",
    "        return math.prod([self.alpha(j) for j in range(t)])\n",
    "\n",
    "    def get_loss(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Corresponds to Algorithm 1 from (Ho et al., 2020).\n",
    "        \"\"\"\n",
    "        # Get a random time step for each image in the batch\n",
    "        ts = torch.randint(0, self.t_range, [batch.shape[0]], device=self.device)\n",
    "        noise_imgs = []\n",
    "        # Generate noise, one for each image in the batch\n",
    "        epsilons = torch.randn(batch.shape, device=self.device)\n",
    "        for i in range(len(ts)):\n",
    "            a_hat = self.alpha_bar(ts[i])\n",
    "            noise_imgs.append(\n",
    "                (math.sqrt(a_hat) * batch[i]) + (math.sqrt(1 - a_hat) * epsilons[i])\n",
    "            )\n",
    "        noise_imgs = torch.stack(noise_imgs, dim=0)\n",
    "        # Run the noisy images through the U-Net, to get the predicted noise\n",
    "        e_hat = self.forward(noise_imgs, ts)\n",
    "        # Calculate the loss, that is, the MSE between the predicted noise and the actual noise\n",
    "        loss = nn.functional.mse_loss(\n",
    "            e_hat.reshape(-1, self.in_size), epsilons.reshape(-1, self.in_size)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def denoise_sample(self, x, t):\n",
    "        \"\"\"\n",
    "        Corresponds to the inner loop of Algorithm 2 from (Ho et al., 2020).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if t > 1:\n",
    "                z = torch.randn(x.shape)\n",
    "            else:\n",
    "                z = 0\n",
    "            # Get the predicted noise from the U-Net\n",
    "            e_hat = self.forward(x, t.view(1).repeat(x.shape[0]))\n",
    "            # Perform the denoising step to take the image from t to t-1\n",
    "            pre_scale = 1 / math.sqrt(self.alpha(t))\n",
    "            e_scale = (1 - self.alpha(t)) / math.sqrt(1 - self.alpha_bar(t))\n",
    "            post_sigma = math.sqrt(self.beta(t)) * z\n",
    "            x = pre_scale * (x - e_scale * e_hat) + post_sigma\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4db2835-e1f4-45cf-bab8-cb5746bb2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "# my_model = DiffusionMode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6a9d9-8113-4e29-9146-54f3083d7265",
   "metadata": {},
   "source": [
    "- `dim`: Initial dimension size for the filters in the U-Net.\n",
    "- `dim_mults`: A tuple representing how the dimensions of the feature maps increase during the downsampling and upsampling.\n",
    "- `channels:` The number of input channels (4 for one-hot encoded SNPs: A, C, G, T).\n",
    "- `prev_dim:` Starts as the number of channels (4 for one-hot encoding) and gets updated as we add layers.\n",
    "- `Downsampling layers:` For each value in dim_mults, we create a 1D convolutional layer (nn.Conv1d) that increases the depth of the feature maps.prev_dim keeps track of the number of channels from the previous layer. kernel_size=3 and padding=1 keep the sequence length intact during convolutions (3x3 kernels).\n",
    "- `Upsampling layers:` This part creates the decoder, where we reduce the depth of the feature maps and aim to recreate the input SNP sequence.\n",
    "- `Reversed dim_mults:` Since we are upsampling, we reverse the order of dim_mults.\n",
    "- Final layer: A 1D convolution to reduce the output back to the number of classes (4), i.e., the SNP categories (A, C, G, T).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22e115-5570-476b-9b0c-0db2e2129d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\" Simple U-Net for SNP denoising. \"\"\"\n",
    "    def __init__(self, dim=64, dim_mults=(0, 0.5, 1, 9), channels=4):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        \n",
    "        prev_dim = channels\n",
    "        for mult in dim_mults:\n",
    "            self.downs.append(nn.Conv1d(prev_dim, dim * mult, kernel_size=3, padding=1))\n",
    "            prev_dim = dim * mult\n",
    "        \n",
    "        for mult in reversed(dim_mults):\n",
    "            self.ups.append(nn.Conv1d(prev_dim, dim * mult, kernel_size=3, padding=1))\n",
    "            prev_dim = dim * mult\n",
    "        \n",
    "        self.final = nn.Conv1d(prev_dim, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        skips = []\n",
    "        for down in self.downs:\n",
    "            x = F.relu(down(x))\n",
    "            skips.append(x)\n",
    "\n",
    "        for up in self.ups:\n",
    "            x = F.relu(up(x + skips.pop()))  # Skip connections\n",
    "\n",
    "        return self.final(x)  # Output logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e18946-c142-4d0f-b62d-60cef8a322bd",
   "metadata": {},
   "source": [
    "- `snp_length:` Length of the SNP sequence.\n",
    "- `t_range:` The number of timesteps used for the diffusion process.\n",
    "- `num_classes:` Number of possible categories for each SNP (A, C, G, T).\n",
    "- `beta_small and beta_large:` Parameters that control the amount of noise added at each timestep in the diffusion process.\n",
    "- `Noise schedule:` The beta(t) function defines the noise level at each timestep. It linearly interpolates between beta_small and beta_large over the range of t.\n",
    "- `q_sample(x, t):` Adds noise to the input SNP sequence x at timestep t.\n",
    "- `(1 - beta_t) * x` keeps the original SNP with probability (1 - beta_t).\n",
    "- `(beta_t / self.num_classes)` represents the small probability of flipping the SNP to any of the other categories (A, C, G, T).\n",
    "- `torch.multinomial(probs.view(-1, self.num_classes), 1):` Samples new SNPs based on the probabilities.\n",
    "- `Forward pass:` The noisy SNP data x is passed through the U-Net to get predicted categorical probabilities.\n",
    "- `get_loss():` Computes the loss function for training. It calculates the categorical cross-entropy between the predicted SNP sequence and the ground truth.\n",
    "- `ts:` Randomly generates timesteps for each sample in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d20eb-a1fc-4bd5-bda7-d9ab26ef1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, snp_length, t_range):\n",
    "        super().__init__()\n",
    "        self.snp_length = snp_length\n",
    "        self.t_range = t_range\n",
    "        self.num_classes = 4  \n",
    "        self.beta_small = 1e-4\n",
    "        self.beta_large = 0.02\n",
    "\n",
    "        self.unet = Unet(dim = 64, dim_mults = (1, 2, 4, 8), channels=img_depth)\n",
    "\n",
    "    def beta(self, t):\n",
    "        \"\"\" Defines the noise schedule: a simple linear interpolation. \"\"\"\n",
    "        return self.beta_small + (t / self.t_range) * (self.beta_large - self.beta_small)\n",
    "\n",
    "    def q_sample(self, x, t):\n",
    "        \"\"\"\n",
    "        Discrete forward process: Replaces SNPs with noise.\n",
    "        x: SNP data (normalized)\n",
    "        t: Time step\n",
    "        \"\"\"\n",
    "        beta_t = self.beta(t)  # Get noise level at time t\n",
    "        probs = (1 - beta_t) * x + (beta_t / self.num_classes)  # Small probability of flipping SNP\n",
    "        x_t = torch.multinomial(probs.view(-1, self.num_classes), 1).view(x.shape)  # Sample new SNPs\n",
    "        return x_t\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\" Predicts categorical probabilities for SNPs. \"\"\"\n",
    "        return self.unet(x, t)\n",
    "\n",
    "    def get_loss(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training loss using categorical cross-entropy.\n",
    "        \"\"\"\n",
    "        ts = torch.randint(0, self.t_range, [batch.shape[0]], device=batch.device)\n",
    "\n",
    "    \n",
    "        # Apply categorical noise\n",
    "        noisy_batch = torch.stack([self.q_sample(batch[i], ts[i]) for i in range(len(ts))])\n",
    "\n",
    "        # Run noisy SNPs through the model\n",
    "        pred_logits = self.forward(noisy_batch, ts)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(pred_logits.permute(0, 2, 1), batch)  # Shape: (batch, 4, SNP_length)\n",
    "        return loss\n",
    "\n",
    "    def denoise_sample(self, x, t):\n",
    "        \"\"\"\n",
    "        Reverse diffusion: Recovers the SNPs step-by-step.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Predict categorical logits\n",
    "            logits = self.forward(x, t.view(1).repeat(x.shape[0]))\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample most likely SNPs\n",
    "            x_t_minus_1 = torch.argmax(probs, dim=-1)  # Take most probable SNP\n",
    "\n",
    "            return x_t_minus_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1fb8b-abdc-4e22-90f2-d66894446458",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = ... # from LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe05a17-fe23-4a38-a1be-d0ae32ecf212",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ... # instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06647f2d-4e84-4b97-91ed-943eb1e653d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
