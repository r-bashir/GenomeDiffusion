# ===== W&B Sweeps Configuration for GenomeDiffusion HPO =====
# Comprehensive hyperparameter optimization for cluster/server
# Full hyperparameter search across all important parameters

# Program to run
program: train_sweep.py

# Optimization method
method: bayes

# Total trials across all agents
run_cap: 50

# Metric to optimize
metric:
  goal: minimize
  name: val_loss_epoch

# Early termination
early_terminate:
  type: hyperband
  min_iter: 10  # More aggressive early stopping
  eta: 5  # Faster elimination of poor trials

# Hyperparameter search space
parameters:

  # === DATA ===
  batch_size:
    values: [16, 32, 64]

  # === MLP ===
  embedding_dim:
    values: [16, 32, 64, 128]

  with_time_emb:
    values: [true, false]

  # === TRAINING ===
  epochs:
    value: 50

  accumulate_grad_batches:
    values: [4, 8, 16]

  # === OPTIMIZER ===
  learning_rate:
    values: [0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005, 0.01]

  weight_decay:
    values: [0.00001, 0.00003, 0.0001, 0.0003, 0.001]

  amsgrad:
    values: [true, false]

  # === SCHEDULER ===
  scheduler_type:
    values: ['cosine', 'warmup_cosine', 'reduce']

  # Cosine scheduler
  eta_min:
    values: [1e-6, 1e-5, 1e-4]

  # WarmupCosine scheduler
  warmup_epochs:
    values: [5, 10]

  # Reduce scheduler
  scheduler_mode:
    values: ['min', 'max']

  scheduler_factor:
    values: [0.3, 0.5, 0.7]

  scheduler_patience:
    values: [5, 10, 15, 20]

  scheduler_threshold:
    values: [1e-6, 1e-5, 1e-4]

  scheduler_min_lr:
    values: [1e-6, 1e-5, 1e-4]

