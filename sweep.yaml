# W&B Sweeps Configuration for GenomeDiffusion HPO
# Comprehensive hyperparameter optimization for cluster/server
# Purpose: Full hyperparameter search across all important parameters

program: train_sweep.py

method: bayes

metric:
  goal: minimize
  name: val_loss_epoch

# Early termination to save compute
early_terminate:
  type: hyperband
  min_iter: 10
  eta: 3

parameters:
  # === OPTIMIZER ===
  learning_rate:
    distribution: log_uniform
    min: 1e-4
    max: 1e-2
  
  weight_decay:
    distribution: log_uniform
    min: 1e-5
    max: 1e-3
  
  # === SCHEDULER ===
  scheduler_type:
    values: ['cosine', 'reduce']
  
  eta_min:
    distribution: log_uniform
    min: 1e-6
    max: 1e-4
  
  scheduler_factor:
    values: [0.3, 0.5, 0.7]
  
  scheduler_patience:
    values: [5, 10, 15, 20]
  
  # === MODEL ARCHITECTURE ===
  embedding_dim:
    values: [16, 32, 64, 128]
  
  dim_mults:
    values: [[1, 2], [1, 2, 4], [1, 2, 4, 8]]
  
  with_time_emb:
    values: [true, false]
  
  with_pos_emb:
    values: [true, false]
  
  edge_pad:
    values: [1, 2, 4]
  
  norm_groups:
    values: [2, 4, 8]
  
  # === DATA ===
  batch_size:
    values: [16, 32, 64]
  
  # === TRAINING ===
  epochs:
    values: [50, 100, 200]
  
  warmup_epochs:
    values: [5, 10, 20]

# Number of runs to execute
count: 50
