# W&B Sweeps Configuration for GenomeDiffusion HPO
# Comprehensive hyperparameter optimization for cluster/server
# Purpose: Full hyperparameter search across all important parameters

program: train_sweep.py

method: bayes

metric:
  goal: minimize
  name: val_loss_epoch

# Early termination to save compute
early_terminate:
  type: hyperband
  min_iter: 5  # More aggressive early stopping
  eta: 2    # Faster elimination of poor trials

parameters:
  # === OPTIMIZER ===
  learning_rate:
    values: [0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005, 0.01]

  weight_decay:
    values: [0.00001, 0.00003, 0.0001, 0.0003, 0.001]

  amsgrad:
    values: [true, false]

  # === SCHEDULER ===
  scheduler_type:
    values: ['cosine', 'reduce']

  eta_min:
    values: [1e-6, 1e-5, 1e-4]

  scheduler_factor:
    values: [0.3, 0.5, 0.7]

  scheduler_patience:
    values: [5, 10, 15, 20]

  # === MODEL ARCHITECTURE ===
  embedding_dim:
    values: [16, 32, 64, 128]

  dim_mults:
    values: [[1, 2], [1, 2, 4], [1, 2, 4, 8]]

  with_time_emb:
    values: [true, false]

  with_pos_emb:
    values: [true, false]

  edge_pad:
    values: [1, 2, 4]

  norm_groups:
    values: [2, 4, 8, 16]

  # === ATTENTION MECHANISM ===
  use_attention:
    values: [true, false]

  attention_heads:
    values: [4, 8, 16]  # More heads for better pattern capture

  attention_dim_head:
    values: [16, 32, 64]

  # === DATA ===
  batch_size:
    values: [16, 32, 64]

  # === TRAINING ===
  epochs:
    value: 50  # More epochs for convergence

  warmup_epochs:
    values: [5, 10]
