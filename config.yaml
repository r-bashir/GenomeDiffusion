# Project name
project_name: "100Test"

# Project output
output_path: "${PROJECT_ROOT}/outputs"

# Dataset
data:
  input_path: "${PROJECT_ROOT}/data/HO_data_filtered/HumanOrigins2067_filtered.parquet"
  seq_length: 1000             # Total length is 160858
  batch_size: 16               # Batch size for DataLoader class
  num_workers: 4               # Number of workers for DataLoader class
  datasplit: [1867, 150, 50]   # Data split, we can set 0 for test dataset
  missing_value: 9             # None, to not handle missing values
  normalize: true              # Normalizing/map data to [0.0, 0.5, 1.0]
  augment: false               # Augment data with staircase structure
  augment_pattern:             # Augment pattern for staircase structure
    - [0, 25, 0.0]
    - [25, 75, 0.5]
    - [75, 100, 1.0]
  mixing: false                # Mix data with staircase structure
  mixing_pattern:              # Mixing pattern for staircase structure
    - [0, 3, 0.0]
    - [3, 7, 0.5]
    - [7, 10, 1.0]
  mixing_interval: 10
  scaling: true                # Scale data further using a scale_factor
  scale_factor: 0.5            # Factor to scale data

# DDPM
diffusion:
  timesteps: 1000              # Diffusion steps
  beta_start: 0.00085          # Start value for linear beta scheduler
  beta_end: 0.02               # End value for linear beta scheduler
  max_beta: 0.999              # Max value for cosine beta scheduler (not needed yet)
  beta_schedule: "cosine"      # Choose between 'linear' or 'cosine'
  denoise_step: 1
  discretize: false

# UNet1D
unet:
  embedding_dim: 64            # Sinusoidal (time, position) embedding dimension
  dim_mults: [1, 2, 4, 8]      # Control model complexity as [1, 2, 4, 8]
  channels: 1                  # Channel dimension, always 1 for 1D SNPs
  with_time_emb: true          # On/off sinusoidal time embeddings
  with_pos_emb: true           # On/off sinusoidal position embeddings
  norm_groups: 8               # Number of groups in GroupNorm layers
  edge_pad: 4                  # Edge padding for boundary preservation
  enable_checkpointing: true   # Enable gradient checkpointing for memory efficiency
  use_attention: false         # Enable multi-head self-attention blocks
  attention_heads: 32          # Number of attention heads
  attention_dim_head: 32       # Dimension per attention head

  attention_window: 512        # Window size (for Sparse Attention)
  num_global_tokens: 64        # No of global tokens (for Sparse Attention)
  dropout: 0.2                 # Dropout for ResNet blocks (for Kenneweg)
  use_scale_shift_norm: true   # Scale-shift norm for ResNet blocks (for Kenneweg)

# Trainer
training:
  epochs: 100                  # Number of epochs to train
  warmup_epochs: 5             # Number of warmup epochs
  early_stopping: false        # Whether to use early stopping
  patience: 10                 # Patience for early stopping
  save_top_k: 1                # Number of top models to save
  logger: "wandb"              # Logger to use
  log_every_n_steps: 5         # More frequent logging for short training
  precision: "16-mixed"        # Mixed precision training (saves ~2Ã— memory)
  accumulate_grad_batches: 4   # Increase gradient accumulation for memory efficiency

# Optimizer
optimizer:
  lr: 0.001                    # Learning rate for AdamW optimizer
  min_lr: 5.0e-06              # Minimum learning rate after warmup
  weight_decay: 3.0e-05        # Weight decay (L2 regularization)
  betas: [0.9, 0.999]          # Beta coefficients for running averages of gradient and its square
  eps: 1.0e-08                 # Term for numerical stability
  amsgrad: false               # Use AMSGrad variant of AdamW

# LR Scheduler
scheduler:
  type: "reduce"               # Either "cosine" (CosineAnnealingLR) or "reduce" (ReduceLROnPlateau)
  eta_min: 1.0e-05             # Minimum learning rate for CosineAnnealingLR
  mode: "max"                  # "min" or "max"
  factor: 0.7                  # Gentler LR reduction for fine-tuning
  patience: 15                 # Shorter patience for fast convergence
  threshold: 1.0e-04           # Smaller threshold for detecting subtle improvements
  min_lr: 1.0e-05              # Lower minimum LR for extended fine-tuning
