# Project name
project_name: "NicDiff"

# Project output
output_path: "${PROJECT_ROOT}/outputs"

# Dataset
data:

  # Dataset Path
  input_path: "${PROJECT_ROOT}/data/HO_data_filtered/HumanOrigins2067_filtered.parquet"

  # Data Loading
  batch_size: 64               # Batch size for DataLoader class
  num_workers: 4               # Number of workers for DataLoader class
  datasplit: [1700, 200, 167]  # Data split for Dataset class

  # Data Preprocessing
  seq_length: 100              # Total length is 160858
  missing_value: 9             # None, to not handle missing values
  normalize: true              # Normalizing/map data to [0.0, 0.5, 1.0]
  scaling: true                # Scale data further using a scale_factor
  scale_factor: 0.5            # Factor to scale data
  augment: true                # Mock data with staircase structure

# Time Sampler
time_sampler:
  tmin: 1                      # Min value for time sampler
  tmax: 1000                   # Max value for time sampler, should match `timesteps`

# DDPM
diffusion:
  timesteps: 1000              # Diffusion steps, should match tmax?
  beta_start: 0.0001           # Start value for linear beta scheduler
  beta_end: 0.02               # End value for linear beta scheduler
  max_beta: 0.999              # Maximum value for cosine beta scheduler
  schedule_type: "cosine"      # Noise schedule type (linear/cosine)
  denoise_step: 1
  discretize: False

# UNet1D
unet:
  embedding_dim: 128           # Sinusoidal (time, position) embedding dimension
  dim_mults: [1, 2, 4, 8]      # Reduced from [1, 2, 4, 8]
  channels: 1                  # Channel dimension, always 1 for 1D SNPs
  with_time_emb: false         # Sinusoidal time embeddings
  with_pos_emb: false          # Sinusoidal position embeddings
  resnet_block_groups: 8       # Number of resnet blocks

# Trainer
training:
  epochs: 500                  # Number of epochs to train
  warmup_epochs: 10            # Number of warmup epochs
  early_stopping: false        # Whether to use early stopping
  patience: 10                 # Patience for early stopping
  save_top_k: 3                # Number of top models to save
  num_samples: 10              # Number of samples to generate
  logger: "wandb"              # Logger to use
  log_every_n_steps: 25        # Log every n steps

# Optimizer
optimizer:
  lr: 1e-4                     # Learning rate for AdamW optimizer
  min_lr: 1e-6                 # Minimum learning rate after warmup
  weight_decay: 0.001          # Weight decay (L2 regularization)
  betas: [0.9, 0.999]          # Beta coefficients for running averages of gradient and its square
  eps: 1e-8                    # Term for numerical stability
  amsgrad: false               # Use AMSGrad variant of AdamW

# LR Scheduler
scheduler:
  type: "cosine"               # Either "cosine" (CosineAnnealingLR) or "reduce" (ReduceLROnPlateau)

  # For CosineLR
  eta_min: 1e-6                # Minimum learning rate for CosineAnnealingLR

  # For ReduceLR
  mode: "min"                  # or "max"
  factor: 0.5                  # LR reduction factor
  patience: 15                 # Epochs with no improvement before reducing LR
  threshold: 1e-4              # Minimum change to qualify as improvement
  min_lr: 1e-6                 # Minimum learning rate for ReduceLROnPlateau
  verbose: false               # Print LR reduction events
