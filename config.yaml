# Project name
project_name: "TestDiff"

# Project output
output_path: "${PROJECT_ROOT}/outputs"

# Dataset
data:
  input_path: "${PROJECT_ROOT}/data/HO_data_filtered/HumanOrigins2067_filtered.parquet"
  seq_length: 1000             # Total length is 160858
  batch_size: 16               # Batch size for DataLoader class
  num_workers: 4               # Number of workers for DataLoader class
  datasplit: [1850, 150, 67]   # Data split, we can set 0 for test dataset
  missing_value: 9             # None, to not handle missing values
  normalize: true              # Normalizing/map data to [0.0, 0.5, 1.0]
  augment: false               # Augment data with staircase structure
  augment_pattern:             # Augment pattern for staircase structure
    - [0, 25, 0.0]
    - [25, 75, 0.5]
    - [75, 100, 1.0]
  mixing: false                # Mix data with staircase structure
  mixing_pattern:              # Mixing pattern for staircase structure
    - [0, 3, 0.0]
    - [3, 7, 0.5]
    - [7, 10, 1.0]
  mixing_interval: 10
  scaling: false               # Scale data further using a scale_factor
  scale_factor: 0.5            # Factor to scale data

# DDPM
diffusion:
  timesteps: 1000              # Diffusion steps, should match tmax?
  beta_start: 0.00085          # Start value for linear beta scheduler
  beta_end: 0.02               # End value for linear beta scheduler
  max_beta: 0.999              # Maximum value for cosine beta scheduler
  beta_schedule: "cosine"      # Noise schedule type (linear/cosine)
  denoise_step: 1
  discretize: False

# UNet1D
unet:
  embedding_dim: 16            # Sinusoidal (time, position) embedding dimension
  dim_mults: [1, 2, 4]         # Control model complexity as [1, 2, 4, 8]
  channels: 1                  # Channel dimension, always 1 for 1D SNPs
  with_time_emb: true          # On/off sinusoidal time embeddings
  with_pos_emb: true           # On/off sinusoidal position embeddings
  norm_groups: 8               # Number of groups in GroupNorm layers
  edge_pad: 2                  # Edge padding for boundary preservation
  use_attention: true          # Enable self-attention blocks
  attention_heads: 4           # Increased heads for better pattern capture
  attention_dim_head: 32       # Dimension per attention head
  dropout: 0.2                 # Dropout rate for ResNet blocks
  use_scale_shift_norm: true   # Use scale-shift normalization for ResNet blocks
  attention_checkpoint: true   # Use gradient checkpointing for attention blocks

# Trainer
training:
  epochs: 100                  # Number of epochs to train
  warmup_epochs: 50            # Number of warmup epochs
  early_stopping: false        # Whether to use early stopping
  patience: 10                 # Patience for early stopping
  save_top_k: 1                # Number of top models to save
  logger: "wandb"              # Logger to use
  log_every_n_steps: 5         # More frequent logging for short training

# Optimizer
optimizer:
  lr: 0.0001                   # Learning rate for AdamW optimizer
  min_lr: 1.0e-05              # Minimum learning rate after warmup
  weight_decay: 0.0001         # Weight decay (L2 regularization)
  betas: [0.9, 0.999]          # Beta coefficients for running averages of gradient and its square
  eps: 1.0e-08                 # Term for numerical stability
  amsgrad: false               # Use AMSGrad variant of AdamW

# LR Scheduler
scheduler:
  type: "reduce"               # Either "cosine" (CosineAnnealingLR) or "reduce" (ReduceLROnPlateau)
  eta_min: 1.0e-05             # Minimum learning rate for CosineAnnealingLR
  mode: "min"                  # or "max"
  factor: 0.7                  # Gentler LR reduction for fine-tuning
  patience: 10                 # Shorter patience for fast convergence
  threshold: 1.0e-05           # Smaller threshold for detecting subtle improvements
  min_lr: 1.0e-07              # Lower minimum LR for extended fine-tuning
