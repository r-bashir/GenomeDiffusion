# Project name
project_name: "MixDiff"

# Project output
output_path: "${PROJECT_ROOT}/outputs"

# Dataset
data:

  # Dataset Path
  input_path: "${PROJECT_ROOT}/data/HO_data_filtered/HumanOrigins2067_filtered.parquet"

  # Data Loading
  batch_size: 16               # Batch size for DataLoader class
  num_workers: 4               # Number of workers for DataLoader class
  datasplit: [1850, 150, 67]   # Data split, we can set 0 for test dataset

  # Data Preprocessing
  seq_length: 1000             # Total length is 160858
  missing_value: 9             # None, to not handle missing values
  normalize: true              # Normalizing/map data to [0.0, 0.5, 1.0]

  augment: false               # Augment data with staircase structure
  augment_pattern:
    - [0, 25, 0.0]
    - [25, 75, 0.5]
    - [75, 100, 1.0]

  mixing: true                 # Mix data with staircase structure
  mixing_pattern:
    - [0, 3, 0.0]
    - [3, 7, 0.5]
    - [7, 10, 1.0]
  mixing_interval: 10

  scaling: true                # Scale data further using a scale_factor
  scale_factor: 0.5            # Factor to scale data

# Time Sampler
time_sampler:
  tmin: 1                      # Min value for time sampler
  tmax: 1000                   # Max value for time sampler, should match `timesteps`

# DDPM
diffusion:
  timesteps: 1000              # Diffusion steps, should match tmax?
  beta_start: 0.0001           # Start value for linear beta scheduler
  beta_end: 0.02               # End value for linear beta scheduler
  max_beta: 0.999              # Maximum value for cosine beta scheduler
  schedule_type: "cosine"      # Noise schedule type (linear/cosine)
  denoise_step: 1
  discretize: False

# UNet1D
unet:
  embedding_dim: 64            # Sinusoidal (time, position) embedding dimension
  dim_mults: [1, 2, 4]         # Control model complexity as [1, 2, 4, 8]
  channels: 1                  # Channel dimension, always 1 for 1D SNPs
  with_time_emb: true          # On/off sinusoidal time embeddings
  with_pos_emb: true           # On/off sinusoidal position embeddings
  norm_groups: 2               # Number of groups in GroupNorm layers
  edge_pad: 2                  # Edge padding for boundary preservation

# Trainer
training:
  epochs: 100                  # Number of epochs to train
  warmup_epochs: 10            # Number of warmup epochs
  early_stopping: false        # Whether to use early stopping
  patience: 10                 # Patience for early stopping
  save_top_k: 1                # Number of top models to save
  logger: "wandb"              # Logger to use
  log_every_n_steps: 5         # More frequent logging for short training

  num_samples: 10              # Number of samples to generate (Why I have it?)

# Optimizer
optimizer:
  lr: 0.001                    # Learning rate for AdamW optimizer
  min_lr: 1.0e-06              # Minimum learning rate after warmup
  weight_decay: 0.001         # Weight decay (L2 regularization)
  betas: [0.9, 0.999]          # Beta coefficients for running averages of gradient and its square
  eps: 1.0e-08                 # Term for numerical stability
  amsgrad: false                # Use AMSGrad variant of AdamW

# LR Scheduler
scheduler:
  type: "cosine"               # Either "cosine" (CosineAnnealingLR) or "reduce" (ReduceLROnPlateau)

  # For CosineLR
  eta_min: 1.0e-05             # Minimum learning rate for CosineAnnealingLR

  # For ReduceLR
  mode: "min"                  # or "max"
  factor: 0.7                  # Gentler LR reduction for fine-tuning
  patience: 10                 # Shorter patience for fast convergence
  threshold: 1e-5              # Smaller threshold for detecting subtle improvements
  min_lr: 1e-7                 # Lower minimum LR for extended fine-tuning
