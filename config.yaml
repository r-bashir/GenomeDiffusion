# Project name
project_name: "100Test"

# Project output
output_path: "${PROJECT_ROOT}/outputs"

# Dataset
data:
  input_path: "${PROJECT_ROOT}/data/HO_data_filtered/HumanOrigins2067_filtered.parquet"
  seq_length: 100              # Total length is 160858
  batch_size: 128              # Batch size for DataLoader class
  num_workers: 4               # Number of workers for DataLoader class
  datasplit: [1867, 150, 50]   # Data split, we can set 0 for test dataset
  missing_value: 9             # None, to not handle missing values
  normalize: true              # Normalizing/map data to [0.0, 0.5, 1.0]
  staircase: false             # Force data to have staircase structure
  staircase_pattern:           # Staircase pattern for staircase structure
    - [0, 25, 0.0]
    - [25, 75, 0.5]
    - [75, 100, 1.0]
  mixing: false                # Mix data with staircase structure
  flip_mixing: false           # Flip mixing pattern on odd-indexed samples, requires mixing=true
  mixing_pattern:              # Mixing pattern for staircase structure
    - [0, 3, 0.0]
    - [3, 7, 0.5]
    - [7, 10, 1.0]
  mixing_interval: 10
  scaling: true                # Scale data further using a scale_factor
  scale_factor: 0.5            # Factor to scale data

# DDPM
diffusion:
  timesteps: 1000              # Diffusion steps
  beta_start: 0.00085          # Start value for linear beta scheduler
  beta_end: 0.02               # End value for linear beta scheduler
  max_beta: 0.999              # Max value for cosine beta scheduler (not needed yet)
  beta_schedule: "cosine"      # Choose between 'linear' or 'cosine'
  denoise_step: 1
  discretize: false

# UNet1D
unet:
  embedding_dim: 512           # Embedding dimension, will be projected to time_dim=128
  dim_mults: [1, 2, 4, 8]      # Control model complexity as [1, 2, 4, 8]
  channels: 1                  # Channel dimension, always 1 for 1D SNPs
  with_time_emb: true          # On/off sinusoidal time embeddings
  time_dim: 128                # Projected time embedding dimension used inside ResnetBlocks
  with_pos_emb: true           # On/off sinusoidal position embeddings
  pos_dim: 128                 # Sinusoidal position embedding dimension before 1x1 projection
  norm_groups: 8               # Number of groups in GroupNorm layers
  edge_pad: 1                  # Edge padding for boundary preservation
  enable_checkpointing: true   # Enable gradient checkpointing for memory efficiency
  strict_resize: false         # If True, any length mismatch is an error (preferred)
  pad_value: 0.0               # Value for zero padding if strict_resize is False
  dropout: 0.2                 # Dropout for ResnetBlocks
  use_scale_shift_norm: true   # Scale-shift norm for ResnetBlocks

  use_attention: false         # Enable multi-head self-attention blocks
  attention_heads: 32          # Number of attention heads
  attention_dim_head: 32       # Dimension per attention head
  attention_window: 512        # Window size (for Sparse Attention)
  num_global_tokens: 64        # No of global tokens (for Sparse Attention)

# Trainer
training:
  epochs: 100                  # Number of epochs to train
  early_stopping: false        # Whether to use early stopping
  patience: 10                 # Patience for early stopping
  save_last: true              # Save last model checkpoint
  save_top_k: 1                # Number of top models to save
  every_n_epochs: 50           # Save checkpoints every N epochs
  logger: "wandb"              # Logger to use
  log_every_n_steps: 5         # More frequent logging for short training
  precision: "16-mixed"        # Mixed precision training (saves ~2Ã— memory)
  accumulate_grad_batches: 8   # Increase gradient accumulation for memory efficiency

# Optimizer
optimizer:
  lr: 0.005                    # Learning rate for AdamW optimizer
  weight_decay: 0.0001         # Weight decay (L2 regularization)
  betas: [0.9, 0.999]          # Beta coefficients for running averages of gradient and its square
  eps: 1e-8                    # Term for numerical stability
  amsgrad: true                # Use AMSGrad variant of AdamW

# LR Schedulers
scheduler:
  type: "reduce"               # type:"cosine", "warmup_cosine", "reduce", or "onecycle"
  eta_min: 1e-5                # cosine: floor or min lr
  warmup_epochs: 10            # warmup_cosine: number of warmup epochs
  mode: "min"                  # reduce: "min" or "max" (use "min" for val_loss)
  factor: 0.3                  # reduce: LR reduction factor
  patience: 20                 # reduce: Patience for ReduceLROnPlateau
  threshold: 1e-5              # reduce: Minimum improvement threshold
  min_lr: 1e-5                 # reduce: Lower LR bound for ReduceLROnPlateau
  max_lr: 0.01                 # onecycle: Peak LR for OneCycleLR
  pct_start: 0.3               # onecycle: Fraction of total steps to increase LR
  anneal_strategy: "cos"       # onecycle: "cos" or "linear"
  final_div_factor: 100        # onecycle: Final LR = max_lr / final_div_factor
