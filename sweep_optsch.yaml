# W&B Sweep: Optimizer + Scheduler Only
# Use this sweep to re-tune a model starting from a checkpoint while keeping
# the architecture and data settings fixed (they come from config.yaml).

program: train_sweep.py
method: bayes
run_cap: 100

metric:
  name: val_loss_epoch
  goal: minimize

# Optional early termination to save compute on poor trials
early_terminate:
  type: hyperband
  min_iter: 10
  eta: 5

parameters:
  # =====================
  # Optimizer parameters
  # =====================
  learning_rate:
    # Base LR for optimizers or schedulers that use it directly (e.g., cosine/reduce)
    # For OneCycleLR the effective base LR is max_lr / div_factor; we still probe base LR for non-OneCycle runs.
    values: [0.00002, 0.00005, 0.0001, 0.0002, 0.0003, 0.0005]

  weight_decay:
    # Medium log-spaced grid appropriate for AdamW on diffusion models
    values: [0.000001, 0.000003, 0.00001, 0.00003, 0.00005, 0.0001, 0.0003, 0.0005, 0.001]

  amsgrad:
    values: [true, false]

  # Additional AdamW hyperparameters
  beta1:
    values: [0.85, 0.9, 0.95]
  beta2:
    values: [0.99, 0.995, 0.999]
  eps:
    values: [1e-8, 3e-8, 1e-7, 3e-7, 1e-6]

  # =====================
  # Scheduler parameters
  # =====================
  scheduler_type:
    values: ['onecycle'] # , 'warmup_cosine', 'cosine', 'reduce']

  # OneCycleLR
  scheduler_max_lr:
    # Peak LR during warmup, medium granularity.
    values: [0.0001, 0.0002, 0.0003, 0.0005, 0.0007, 0.001]

  scheduler_pct_start:
    # Fraction of steps in warmup phase (long cosine tail)
    values: [0.2, 0.25, 0.3, 0.35, 0.4]

  scheduler_anneal_strategy:
    # Annealing strategy for OneCycleLR
    values: ['cos', 'linear']

  scheduler_div_factor:
    # initial_lr = max_lr / div_factor
    values: [10.0, 15.0, 20.0, 25.0, 30.0]

  scheduler_final_div_factor:
    # final_lr = initial_lr / final_div_factor
    values: [50, 100, 200, 400, 800]

  # WarmupCosineLR
  #warmup_epochs:
  #  values: [5, 10, 15]

  # CosineAnnealingLR
  #eta_min:
  #  values: [1e-7, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]

  # ReduceLROnPlateau
  #scheduler_mode:
  #  values: ['min']

  #scheduler_factor:
  #  values: [0.2, 0.3, 0.5, 0.7, 0.8]

  #scheduler_patience:
  #  values: [3, 5, 8, 10, 15, 20, 30]

  #scheduler_threshold:
  #  values: [1e-6, 1e-5, 1e-4, 1e-3]

  #scheduler_min_lr:
  #  values: [1e-7, 1e-6, 1e-5]
