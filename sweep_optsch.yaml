# W&B Sweep: Optimizer + Scheduler Only
# Use this sweep to re-tune a model starting from a checkpoint while keeping
# the architecture and data settings fixed (they come from config.yaml).

program: train_sweep.py
method: bayes
run_cap: 40

metric:
  name: val_loss_epoch
  goal: minimize

# Optional early termination to save compute on poor trials
early_terminate:
  type: hyperband
  min_iter: 10
  eta: 5

parameters:
  # =====================
  # Optimizer parameters
  # =====================
  learning_rate:
    values: [0.00002, 0.00005, 0.00007, 0.0001, 0.0002, 0.0003, 0.0005, 0.0007, 0.001, 0.002]

  weight_decay:
    values: [0.000001, 0.000003, 0.00001, 0.00003, 0.00005, 0.0001, 0.0003, 0.0005, 0.001]

  amsgrad:
    values: [true, false]

  # =====================
  # Scheduler parameters
  # =====================
  scheduler_type:
    values: ['cosine', 'warmup_cosine', 'reduce', 'onecycle']

  # Cosine scheduler
  eta_min:
    values: [1e-7, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]

  # WarmupCosine scheduler
  warmup_epochs:
    values: [2, 3, 5, 8, 10, 15]

  # ReduceLROnPlateau
  scheduler_mode:
    values: ['min']
  scheduler_factor:
    values: [0.2, 0.3, 0.5, 0.7, 0.8]
  scheduler_patience:
    values: [3, 5, 8, 10, 15, 20, 30]
  scheduler_threshold:
    values: [1e-6, 1e-5, 1e-4, 1e-3]
  scheduler_min_lr:
    values: [1e-7, 1e-6, 1e-5]

  # OneCycleLR
  scheduler_max_lr:
    values: [0.0005, 0.001, 0.002, 0.003, 0.005]
  scheduler_pct_start:
    values: [0.05, 0.1, 0.15, 0.2, 0.25, 0.35, 0.45]
  scheduler_anneal_strategy:
    values: ['cos', 'linear']
  scheduler_final_div_factor:
    values: [10, 25, 50, 100, 200, 400, 1000]
